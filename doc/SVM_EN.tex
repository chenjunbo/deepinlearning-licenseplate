\documentclass{article}
\usepackage{amsmath}
\usepackage{hyperref}

\title{Support Vector Machine (SVM)}
\author{}
\date{}

\begin{document}
	
	\maketitle
	
	\section{Support Vector Machine (SVM)}
	
	SVM (Support Vector Machine) is a powerful machine learning algorithm used for \textbf{classification} and \textbf{regression}, particularly well-suited for handling complex, nonlinear data and high-dimensional problems. SVM excels at classification tasks, especially in small sample and high-dimensional datasets.
	
	\section{Basic Concepts of SVM}
	
	\begin{itemize}
		\item \textbf{Support Vectors}: In the classification process, support vectors are the data points closest to the decision boundary (hyperplane). They determine the margin of the classifier.
		\item \textbf{Hyperplane}: SVM tries to find a hyperplane in a multi-dimensional space that separates the data points into different classes. This hyperplane serves as the classifier, aiming to separate data points from different categories as widely as possible.
		\item \textbf{Maximum Margin}: The goal of SVM is to find the hyperplane that maximizes the margin between different classes, which improves the generalization ability of the model.
	\end{itemize}
	
	\section{How SVM Works}
	
	\subsection{Linearly Separable Case}
	For linearly separable data, SVM aims to find a hyperplane that completely separates two classes. It optimizes the decision boundary by maximizing the margin between the support vectors and the hyperplane.
	
	\subsection{Non-Linearly Separable Case}
	When data cannot be separated by a linear hyperplane, SVM handles it in two ways:
	
	\begin{itemize}
		\item \textbf{Soft Margin}: Allows some data points to lie on the wrong side of the hyperplane but controls the number of misclassified points through a penalty term.
		\item \textbf{Kernel Trick}: Maps the data from a low-dimensional space to a higher-dimensional space, where a linear hyperplane can be used to separate the data.
	\end{itemize}
	
	\section{Kernel Trick}
	
	One of SVM’s key features is the use of kernel functions, which allows it to find decision boundaries in non-linear data. Common kernel functions include:
	
	\begin{itemize}
		\item \textbf{Linear Kernel}: Used for linearly separable data, typically in simple binary classification problems.
		\item \textbf{Polynomial Kernel}: Handles non-linear data, where the degree of the polynomial controls the complexity of the model.
		\item \textbf{Radial Basis Function (RBF) Kernel}: Commonly used for complex, non-linear data, effective in most classification tasks.
		\item \textbf{Sigmoid Kernel}: In some cases, behaves similarly to a neural network.
	\end{itemize}
	
	\section{Types of SVM}
	
	\begin{itemize}
		\item \textbf{Classification (SVC, Support Vector Classification)}: SVM is widely used for classification tasks, where it tries to find a hyperplane to separate different categories of data points.
		\item \textbf{Regression (SVR, Support Vector Regression)}: SVM can also be used for regression tasks, predicting continuous values. In SVR, the model adjusts the support vectors to minimize the regression error.
		\item \textbf{Multiclass Classification}: Although SVM is inherently a binary classifier, it can handle multiclass problems using strategies such as "One-vs-Rest" or "One-vs-One".
	\end{itemize}
	
	\section{Advantages and Disadvantages of SVM}
	
	\subsection{Advantages}
	
	\begin{itemize}
		\item \textbf{Handles High-Dimensional Data}: SVM performs well in high-dimensional spaces and can handle cases where the number of features exceeds the number of samples.
		\item \textbf{Prevents Overfitting}: By maximizing the margin between classes, SVM has strong generalization capabilities.
		\item \textbf{Flexible Kernel Functions}: SVM can handle both linear and non-linear data by choosing appropriate kernel functions.
	\end{itemize}
	
	\subsection{Disadvantages}
	
	\begin{itemize}
		\item \textbf{Inefficient for Large Datasets}: SVM can be slow when dealing with large datasets, especially in training time.
		\item \textbf{Sensitive to Parameters and Kernel Choice}: The choice of C and kernel parameters significantly affects the model's performance, making it difficult to tune.
		\item \textbf{Complexity in Multiclass Classification}: Handling multiclass problems requires extensions, which increases the model's complexity.
	\end{itemize}
	
	\section{SVM Parameters}
	
	\begin{itemize}
		\item \textbf{C Parameter}: C is the penalty parameter that controls the trade-off between maximizing the margin and minimizing classification errors. A larger C reduces misclassification but can lead to overfitting; a smaller C allows more misclassification but increases the margin.
		\item \textbf{$γ$ Parameter (Gamma)}: Used primarily with non-linear kernels like RBF. A larger γ value makes the model fit the training data more closely, potentially leading to overfitting, while a smaller γ leads to a smoother decision boundary, potentially underfitting.
	\end{itemize}
	
	\section{Applications of SVM}
	
	\begin{itemize}
		\item \textbf{Image Classification}: Used in tasks like handwritten digit recognition, face recognition, etc.
		\item \textbf{Text Classification}: Applied in tasks like spam filtering and sentiment analysis.
		\item \textbf{Bioinformatics}: Utilized for gene expression data classification, protein classification, and more.
		\item \textbf{Financial Forecasting}: Can be used for predicting market trends, risk assessment, etc.
	\end{itemize}
	
	\section{Conclusion}
	
	SVM is a powerful supervised learning algorithm suitable for classification tasks involving small samples and high-dimensional data. By using different kernel functions, SVM can handle both linear and non-linear data. Although it may struggle with efficiency in large datasets, SVM demonstrates excellent performance in many real-world applications, particularly in classification tasks, thanks to its strong generalization capabilities.
	
\end{document}