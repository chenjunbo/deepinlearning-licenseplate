\documentclass{article}
\usepackage{amsmath}
\usepackage{hyperref}

\title{Artificial Neural Network - Multilayer Perceptron (ANN-MLP)}
\author{}
\date{}

\begin{document}
	
	\maketitle
	
	\section{Introduction}
	An Artificial Neural Network (ANN) is a computational model inspired by the way biological neural networks in the human brain process information. \textbf{Multilayer Perceptron (MLP)} is a type of ANN composed of multiple layers of neurons that are fully connected, making it a powerful tool for both classification and regression tasks.
	
	\section{Structure of MLP}
	
	\subsection{Layers}
	\begin{itemize}
		\item \textbf{Input Layer}: The input layer consists of neurons that receive the input features from the dataset. Each feature corresponds to one neuron in this layer.
		\item \textbf{Hidden Layers}: Between the input and output layers, MLP can have one or more hidden layers. These layers are responsible for learning the complex relationships in the data. Each neuron in a hidden layer takes input from all neurons in the previous layer and passes it through an activation function.
		\item \textbf{Output Layer}: This layer provides the final output of the network. For classification tasks, the output is usually in the form of probabilities (softmax activation). For regression, the output is a continuous value.
	\end{itemize}
	
	\subsection{Neurons}
	Each neuron in an MLP performs two key steps:
	\begin{itemize}
		\item \textbf{Weighted Sum}: Each neuron computes the weighted sum of its inputs.
		\begin{equation}
			z = \sum (w_i \cdot x_i) + b
		\end{equation}
		where:
		\begin{itemize}
			\item \( w_i \) are the weights,
			\item \( x_i \) are the input values,
			\item \( b \) is the bias.
		\end{itemize}
		\item \textbf{Activation Function}: The result of the weighted sum is passed through an activation function to introduce non-linearity, enabling the model to capture complex patterns in data. Common activation functions include:
		\begin{itemize}
			\item \textbf{Sigmoid}: Converts the input into a value between 0 and 1.
			\item \textbf{ReLU (Rectified Linear Unit)}: Sets negative inputs to zero and leaves positive inputs unchanged.
			\item \textbf{Tanh}: Converts the input to a value between -1 and 1.
		\end{itemize}
	\end{itemize}
	
	\section{How MLP Works}
	
	\subsection{Forward Propagation}
	During forward propagation, the input data is passed through the network layer by layer. Each neuron in one layer computes its output and passes it to the neurons in the next layer until the final output is produced.
	
	\subsection{Backpropagation and Training}
	MLP learns through a process called backpropagation. Here's how it works:
	\begin{itemize}
		\item \textbf{Loss Calculation}: The network's prediction is compared to the actual target using a loss function (e.g., mean squared error for regression or cross-entropy for classification).
		\item \textbf{Gradient Descent}: To minimize the loss, the network adjusts its weights and biases by computing the gradients of the loss function with respect to each weight using the chain rule.
		\item \textbf{Weight Update}: The weights are updated in the direction that reduces the loss, and this process is repeated iteratively until the model converges to an optimal solution.
	\end{itemize}
	
	\section{Key Hyperparameters}
	\begin{itemize}
		\item \textbf{Number of Layers and Neurons}: The depth (number of layers) and width (number of neurons per layer) significantly affect the model's capacity to learn.
		\item \textbf{Learning Rate}: Controls how much the weights are updated during training. A small learning rate makes learning slower but can prevent overshooting, while a high learning rate can speed up learning but risks missing the optimal solution.
		\item \textbf{Batch Size}: The number of samples processed before updating the weights.
		\item \textbf{Epochs}: The number of times the entire training data is passed through the network.
	\end{itemize}
	
	\section{Advantages and Disadvantages of MLP}
	
	\subsection{Advantages}
	\begin{itemize}
		\item \textbf{Universal Approximation}: MLP, with sufficient layers and neurons, can approximate any continuous function, making it a versatile model for various tasks.
		\item \textbf{Non-linearity}: Activation functions allow MLP to model complex, non-linear relationships.
		\item \textbf{Scalability}: MLP can be scaled to handle large datasets by increasing the network size.
	\end{itemize}
	
	\subsection{Disadvantages}
	\begin{itemize}
		\item \textbf{Computational Cost}: MLPs can become computationally expensive as the number of layers and neurons increases.
		\item \textbf{Risk of Overfitting}: Without proper regularization, MLPs are prone to overfitting, especially with small datasets.
		\item \textbf{Requires a Large Dataset}: MLPs typically need a significant amount of data to train effectively.
	\end{itemize}
	
	\section{Common Applications of MLP}
	\begin{itemize}
		\item \textbf{Image Recognition}: MLPs can classify images, though more specialized architectures like CNNs are often used for this task.
		\item \textbf{Natural Language Processing}: MLPs can be used for tasks like text classification, sentiment analysis, etc.
		\item \textbf{Financial Forecasting}: Used for predicting trends in financial markets.
		\item \textbf{Speech Recognition}: Can be applied to speech-to-text systems, though RNNs are more commonly used for sequence data.
	\end{itemize}
	
	\section{Conclusion}
	MLP is a fundamental architecture in neural networks, capable of handling both classification and regression tasks. It can model complex, non-linear relationships in data by learning through backpropagation and gradient descent. Although it has limitations, such as potential overfitting and high computational cost, it remains a widely-used and powerful model in many applications.
	
\end{document}